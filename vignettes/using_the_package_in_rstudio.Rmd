---
title: "Using the Package"
author: "Radical Open Science Syndicate"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using the Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
knitr::opts_chunk$set(
  eval = FALSE,
  echo = TRUE
)
```

## Overview

This vignette demonstrates a comprehensive workflow for processing water quality sensor data (HydroVu API), adding relevant field notes (mWater API), applying quality assurance and quality control (QAQC) flags, and preparing the data for analysis. The workflow combines automated data retrieval, statistical analysis, and multi-level flagging to ensure data quality.

### Output Data Structure

The final dataset contains the following key columns:

`DT_round`: Datetime rounded to 15-minute intervals
`site`: Site identifier
`parameter`: Measured parameter (e.g., "Temperature", "Dissolved Oxygen")
`mean`: Average value for the 15-minute time interval
`units`: Parameter units
`n_obs`: Number of observations in the 15-minute interval
`spread`: Measure of data variability in the 15-minute interval
`auto_flag`: Automated quality control flag
`mal_flag`: Manual malfunction flag (based on field notes)
`sonde_moved`: Indicator of whether sonde was shifted in housing or up/downstream
`season`: Season classification for the measurement

### Memory and Performance Considerations

This workflow is designed to handle large datasets efficiently:

Parallel processing: Uses multiple CPU cores for faster computation
Chunking: Processes data in manageable chunks to avoid memory issues
Garbage collection: Explicitly frees memory between processing steps
Efficient data structures: Uses data.table for fast operations

### Customization
You can customize the workflow by:

Modifying the sites_to_process vector to include/exclude specific sites
Adjusting the date range for data retrieval
Changing the summarization interval in `tidy_api_data()`
Adding custom threshold files for different parameters or seasons
Modifying chunk sizes for different hardware configurations

## Configuration

First, customize your directory paths and processing parameters:

```{r}
# Configure your directory paths
staging_directory <- "path/to/your/raw_data"          # Where raw data will be stored
flagged_directory <- "path/to/your/flagged_data"      # Where flagged data will be saved
temp_directory <- "path/to/your/temp_files"          # Temporary processing files
final_directory <- "path/to/your/final_output"       # Final processed data

# Configure your threshold files
sensor_thresholds_file <- "path/to/sensor_spec_thresholds.yml"
seasonal_thresholds_file <- "path/to/updated_seasonal_thresholds_2025.csv"

# Configure your credentials files
mwater_creds_file <- "path/to/mWaterCreds.yml"
hydrovu_creds_file <- "path/to/HydroVuCreds.yml"

# Configure date range for data retrieval
start_date <- "2025-07-01 00:00:00"  # MST
end_date <- "2025-07-05 23:59:59"    # MST

# Configure sites to process
sites_to_process <- c("archery", "bellvue", "boxcreek", "boxelder", "cbri", "chd", 
                      "cottonwood", "elc", "joei", "lbea", "legacy", "lincoln", 
                      "mtncampus", "pbd", "pbr", "penn", "pfal", "pman", "prospect", 
                      "river bluffs", "riverbluffs", "riverbend", "salyer", "sfm", 
                      "springcreek", "tamasag", "timberline", "udall")

# Configure parallel processing
max_workers <- 4  # Maximum number of parallel workers
```


## Package Setup

The workflow uses several R packages and custom functions in {fcw.qaqc}. Here's how to set up the environment:

```{r}
# load required packages
package_loader <- function(x) {
  if (x %in% installed.packages()) {
    suppressMessages({
      library(x, character.only = TRUE)
    })
  } else {
    suppressMessages({
      install.packages(x)
      library(x, character.only = TRUE)
    })
  }
}

# load all required packages
invisible(
  lapply(c("arrow", "data.table", "httr2", "tidyverse", "lubridate", "zoo", 
           "padr", "stats", "RcppRoll", "yaml", "here", "furrr", "fcw.qaqc"),
         package_loader)
)

# set up parallel processing
num_workers <- min(availableCores() - 1, max_workers)
plan(multisession, workers = num_workers)
furrr_options(
  globals = TRUE,
  packages = c("arrow", "data.table", "httr2", "tidyverse", "lubridate", "zoo",
               "padr", "stats", "RcppRoll", "yaml", "here", "fcw.qaqc")
)

# suppress scientific notation for consistent formatting
options(scipen = 999)
```

## Step 1: Loading In Data

Load threshold data and API credentials:

```{r}
# read threshold data
sensor_thresholds <- read_yaml(sensor_thresholds_file)
season_thresholds <- read_csv(seasonal_thresholds_file, show_col_types = FALSE) %>%
  fix_site_names()

# read API credentials
mWater_creds <- read_yaml(mwater_creds_file)
hv_creds <- read_yaml(hydrovu_creds_file)

# authenticate access to HydroVu API
hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]))
```

## Step 2: Retrieve Field Notes and Metadata

The `load_mWater()` function retrieves field data and sonde maintenance notes:

```{r}
# pull field data from mWater API
mWater_data <- load_mWater(creds = mWater_creds)

# grab field notes with proper timezone handling
all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data) %>%
  mutate(DT_round = with_tz(DT_round, tzone = "UTC"),
         last_site_visit = with_tz(last_site_visit, tzone = "UTC"),
         DT_join = as.character(DT_round))

# grab sensor malfunction records
sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data) %>%
  mutate(start_DT = with_tz(start_DT, tzone = "UTC"),
         end_DT = with_tz(end_DT, tzone = "UTC"))

# grab cleaning records
sensor_cleaning_notes <- grab_mWater_cleaning_notes(mWater_api_data = mWater_data)
```

## Step 3: Retrieve Sensor Data

Pull sensor data from HydroVu API for all specified sites:

```{r}
# get HydroVu site information
hv_sites <- hv_locations_all(hv_token) %>%
  filter(!grepl("vulink", name, ignore.case = TRUE))

# convert dates to proper timezone
mst_start <- ymd_hms(start_date, tz = "America/Denver")
mst_end <- ymd_hms(end_date, tz = "America/Denver")

# pull data for each site
walk(sites_to_process,
     function(site) {
       message("Requesting HV data for: ", site)
       api_puller(
         site = site,
         start_dt = with_tz(mst_start, tzone = "UTC"),
         end_dt = with_tz(mst_end, tzone = "UTC"),
         api_token = hv_token,
         dump_dir = staging_directory
       )
     }
)
```

## Step 4: Load and Preprocess Raw Data

The `api_puller()` function saves data as parquet files. Now we load and combine all files using `munge_api_data()`:

```{r}

# preprocess data
hv_data <- munge_api_data(api_dir = staging_directory)
# standardize and remove unwanted data(Vulink, virridy and FDOM Fluorescence)
hv_data_standardized <- hv_data%>%
  # Filter out VuLink data (not used in CSU/FCW networks)
  dplyr::filter(!grepl("vulink", name, ignore.case = TRUE)) %>%
  # Filter out Virridy data (not used in CSU/FCW networks)
  dplyr::filter(!grepl("virridy", name, ignore.case = TRUE)) %>%
  # Remove FDOM Fluorescence data (not used in CSU/FCW networks)
  dplyr::filter(!grepl("FDOM Fluorescence", parameter, ignore.case = TRUE)) %>%
  # Remove the equipment name column
  select(-name)%>%
  fix_site_names() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::discard(~ nrow(.) == 0)

```

## Step 5: Data Tidying and Summarization

The `tidy_api_data()` function cleans and summarizes data at 15-minute intervals:

```{r}
# tidy raw data (default 15-minute intervals)
tidy_data <- hv_data_standardized %>%
  future_map(~tidy_api_data(api_data = .), .progress = TRUE) %>%
  keep(~!is.null(.))

# add field notes to tidied data
combined_data <- tidy_data %>%
  future_map(~add_field_notes(df = ., notes = all_field_notes), .progress = TRUE)

# generate summary statistics
summarized_data <- combined_data %>%
  map(~generate_summary_statistics(.))
```

## Step 6: Single-Parameter Quality Control Flags

Apply individual parameter flags using various QAQC functions:

```{r}
# process data in chunks for memory efficiency
summarized_data_chunks <- split(1:length(summarized_data),
                                ceiling(seq_along(1:length(summarized_data))/10))

single_sensor_flags <- list()
for (chunk_idx in seq_along(summarized_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(summarized_data_chunks), " ===")
  
  indices <- summarized_data_chunks[[chunk_idx]]
  chunk_data <- summarized_data[indices]
  
  # apply single-parameter flags
  chunk_results <- chunk_data %>%
    future_map(
      function(data) {
        flagged_data <- data %>%
          data.table(.) %>%
          # flag field visits
          add_field_flag(df = .) %>%
          # flag missing/NA values
          add_na_flag(df = .) %>%
          # flag dissolved oxygen noise patterns
          find_do_noise(df = .) %>%
          # flag repeating/stuck values
          add_repeat_flag(df = .) %>%
          # flag depth shifts (sonde movement)
          add_depth_shift_flag(df = ., level_shift_table = all_field_notes, post2024 = TRUE) %>%
          # flag sensor drift (FDOM, Chl-a, Turbidity)
          add_drift_flag(df = .)
        
        # apply sensor specification flags if thresholds exist
        if (unique(data$parameter) %in% names(sensor_thresholds)) {
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_spec_flag(df = ., spec_table = sensor_thresholds)
        }
        
        # apply seasonal threshold flags if available
        if (unique(data$parameter) %in% unique(season_thresholds$parameter)) {
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_seasonal_flag(df = ., threshold_table = season_thresholds)
        }
        
        return(flagged_data)
      },
      .progress = TRUE
    )
  
  single_sensor_flags <- c(single_sensor_flags, chunk_results)
  
  if (chunk_idx < length(summarized_data_chunks)) {
    gc()  # garbage collection between chunks
    Sys.sleep(0.1)
  }
}
```

## Step 7: Inter-Parameter Quality Control Flags

Apply flags that consider relationships between multiple parameters:

```{r}
# aombine single-parameter flags by site
intrasensor_flags <- single_sensor_flags %>%
  rbindlist(fill = TRUE) %>%
  split(by = "site")

# process inter-parameter flags in chunks
intrasensor_data_chunks <- split(1:length(intrasensor_flags),
                                 ceiling(seq_along(1:length(intrasensor_flags))/3))

intrasensor_flags_list <- list()
for (chunk_idx in seq_along(intrasensor_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(intrasensor_data_chunks), " ===")
  
  indices <- intrasensor_data_chunks[[chunk_idx]]
  chunk_data <- intrasensor_flags[indices]
  
  chunk_results <- chunk_data %>%
    map(
      function(data) {
        flagged_data <- data %>%
          data.table() %>%
          # flag when water temperature below freezing
          add_frozen_flag(.) %>%
          # check for overlapping flags and resolve
          intersensor_check(.) %>%
          # flag potential sensor burial
          add_burial_flag(.) %>%
          # flag when sonde is above water surface
          add_unsubmerged_flag(.)
        
        return(flagged_data)
      }
    ) %>%
    rbindlist(fill = TRUE) %>%
    mutate(flag = ifelse(flag == "", NA, flag)) %>%
    split(f = list(.$site, .$parameter), sep = "-") %>%
    purrr::discard(~ nrow(.) == 0) %>%
    # add known sensor malfunction periods
    map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))
  
  intrasensor_flags_list <- c(intrasensor_flags_list, chunk_results)
  
  if (chunk_idx < length(intrasensor_data_chunks)) {
    gc()
    Sys.sleep(0.1)
  }
}

# save intermediate results
iwalk(intrasensor_flags_list, 
      ~write_csv(.x, file.path(temp_directory, paste0(.y, ".csv"))))
```

## Step 8: Network-Level Quality Control

Apply network-wide checks that compare data across multiple sites:

### Defining a site order 

See `load_site_order()` for details on formatting. Here we load from a YAML file:

```{r}
site_order_list <- load_site_order("templates/template_site_orders.yaml")
#Or from CSV
#site_order_list <- load_site_order("templates/template_site_orders.csv")
```

## Applying network-level checks

```{r}
# apply network-level quality control
network_flags <- intrasensor_flags_list %>%
  # network check compares patterns across sites
  purrr::map(~network_check(df = ., intrasensor_flags_arg = intrasensor_flags_list,
                            site_order_arg = site_order_list)) %>% # using site order list from above to determine network connectivity
  rbindlist(fill = TRUE) %>%
  # clean up flag column formatting
  tidy_flag_column() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  # add suspect data flags for isolated anomalies
  purrr::map(~add_suspect_flag(.)) %>%
  rbindlist(fill = TRUE)
```

## Step 9: Final Data Preparation

Clean and prepare the final dataset:

```{r}
# final data cleaning and preparation
v_final_flags <- network_flags %>%
  # Remove isolated suspect flags (single point anomalies)
  dplyr::mutate(auto_flag = ifelse(
    is.na(auto_flag), NA,
    ifelse(auto_flag == "suspect data" & 
           is.na(lag(auto_flag, 1)) & 
           is.na(lead(auto_flag, 1)), NA, auto_flag)
  )) %>%
  # select final columns
  dplyr::select(c("DT_round", "DT_join", "site", "parameter", "mean", "units", 
                  "n_obs", "spread", "auto_flag", "mal_flag", "sonde_moved",
                  "sonde_employed", "season", "last_site_visit")) %>%
  # clean up empty flags
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA, 
                                   ifelse(auto_flag == "", NA, auto_flag))) %>%
  # split back into site-parameter combinations
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)

# save final processed data
iwalk(v_final_flags, 
      ~write_csv(.x, file.path(final_directory, paste0(.y, ".csv"))))
```

## List of Functions

### Single-Parameter Flags

`add_field_flag()`: Flags data during field maintenance visits
`add_na_flag()`: Flags missing or invalid data points
`find_do_noise()`: Identifies noise patterns in dissolved oxygen data
`add_repeat_flag()`: Flags stuck or repeating sensor values
`add_depth_shift_flag()`: Detects when sensors are moved within their housing
`add_drift_flag()`: Identifies sensor drift in optical sensors
`add_spec_flag()`: Flags values outside manufacturer specifications
`add_seasonal_flag()`: Flags values outside seasonal expectations
`add_suspect_flag()`: Flags isolated anomalies
`add_malfunction_flag()`: Applies known malfunction periods

### Inter-Parameter Flags

`add_frozen_flag()`: Flags periods when water is frozen
`intersensor_check()`: Resolves conflicting flags between parameters
`add_burial_flag()`: Identifies when sensors are buried in sediment
`add_unsubmerged_flag()`: Flags when sensors are above water surface

### Network-Level Flags

`network_check()`: Compares patterns across multiple sites
