---
title: "fixing_calibrations"
output: html_document
---

# Introduction

This vignette demonstrates the complete workflow for back-calibrating water 
quality sensor data using calibration information extracted from HTML calibration 
files. The process corrects sensor drift and applies temporal interpolation 
between calibrations to produce accurate time series data.

# Prepare Environment
```{r setup}
knitr::opts_chunk$set(
  eval = FALSE,
  echo = TRUE
)
```

```{r}
#Installing and loading all packages
invisible(
  lapply(c(
    "tidyverse", # Data manipulation
    "janitor", # Clean dirty data
    "lubridate", # Date-Time Manipulation
    "rvest", # HTML Retrieval and Manipulation
    "readxl", # Reading excel files
    "here", # Easy, cross platform file referencing
    "ggplot2", # Plotting libraries
    "ggpubr",
    "plotly",
    "devtools", # For downloading GitHub packages
    "remotes",
    "yaml"
  ),
  function(x) {
    if (x %in% installed.packages()) {
      suppressMessages({
        library(x, character.only = TRUE)
      })
    } else {
      suppressMessages({
        install.packages(x)
        library(x, character.only = TRUE)
      })
    }
  })
)

load_all()
#library non CRAN packages
# lapply(c("rossyndicate/fcw.qaqc"), function(x) {
#   pack_name <- unlist(strsplit(x, "/"))[2] #get package name (no GH username)
#   
#   if (pack_name %in% installed.packages()) {
#     suppressMessages({
#       library(pack_name, character.only = TRUE)
#     })
#   } else {
#     suppressMessages({
#       devtools::install_github(x)
#       library(pack_name, character.only = TRUE)
#     })
#   }
# })
```

The environment setup loads essential packages for data manipulation and 
sources all calibration functions needed for the workflow.

# Load Calibration Data

UPDATE (JPD): This should get renamed. load calibration data specifically pulls/updates the
calibration data with the site metadata, but for this solution I am making a calibration
data that is filtered to make sensor specific calibration provenance objects that 
we can use to determine which calibrations are tied to which sensors (rather than sites).

```{r load-cal-data}
load_calibration_data(
  cal_data_file_path = here("..", "poudre_sonde_network", "data", "calibration_reports", "0_cal_data_munge", "munged_calibration_data.RDS")
)
```

The `load_calibration_data()` function loads the calibration coefficients and 
drift correction information which were extracted from the HTML calibration files,
if they exist within the file path which is provided to it. This file path, by
default, is `here("data", "calibration_reports", "0_cal_data_munge", "munged_calibration_data.RDS")`

If the data does not exist, or the `update` option is set to `TRUE`, this function 
calls the `cal_extract_markup_data()`, which generates this calibration data 
fresh from raw HTML files from the directory provided to it. By default these 
directories are: `field_cal_dir = here("data", "calibration_reports")` and
`benchtop_cal_dir = here("data", "calibration_reports", "benchtop_calibrations")`.
Though, note that the arguments for `cal_extract_markup_data()` can be changed 
within `load_calibration_data()` via the `...` arguments. Note that `cal_extract_markup_data()`
takes a minute to finish running.

## Calibration Data Structure

The resulting `calibration_data` object follows an organizational structure 
which contains a nested list organized by year, where each year 
contains site-parameter combinations (e.g., "lincoln-pH", "tamasag-Turbidity"). 
Each site-parameter entry includes calibration coefficients (slope and offset values) 
and drift correction measurements (pre- and post-calibration standard readings),
along with their related identification and datetime information.

UPDATE (JPD): I will definitely incorporate the following code into one of the functions here (probably the 
data generation one), but I wanted to draft here to see what I could do with what
I had already. 

# Extract Sensor Specific Calibration Information
```{r}
cal_provenance <- calibration_data  %>% 
  map_dfr(function(year){
    calibrated_site_param_list <- year %>%
      map_dfr(function(site_param){
        site_param %>% 
          select(sensor, sensor_serial, sensor_date, calibration_coefs)
      })
  })

# Lets see if this is good enough for now
cal_provenance_list <- cal_provenance %>%
  split(f = list(.$sensor, .$sensor_serial), sep = "-") %>%
  discard(~is.null(.)||nrow(.) == 0) %>%
  map(function(sensor_df){
    sensor_df %>%
      mutate(
        sensor_date_lead = dplyr::lead(sensor_date, 1),
        calibration_coefs_lead = dplyr::lead(calibration_coefs, 1)
      ) %>%
      # We are going to remove calibration_coefs here to avoid weird stuff down the line
      select(-calibration_coefs)
  }) 

cal_provenance_df <- bind_rows(cal_provenance_list)
```


# Load Sensor Data

```{r load-snsr-data}
# Read in 2023, 2024, and 2025 sensor data
sensor_data <- read_rds(here("..", "poudre_sonde_network", "data", "manual_data_verification", "complete_dataset", "23_25_hv_pull"))

# Commenting all this out. I think there is a better data source somewhere. -JPD

# if (file.exists(here("data", "manual_data_verification", "complete_dataset", "23_25_hv_pull"))) {
#   sensor_data <- read_rds(here("..", "poudre_sonde_network", "data", "manual_data_verification", "complete_dataset", "23_25_hv_pull")) %>% 
#     set_names(names(calibration_data)) %>% 
#     compact()
# } else {
#   library(fcw.qaqc)
#   
#   sensor_data <- names(calibration_data) %>% 
#     map(function(year){
#       
#       # Grab the site for the year from the calibration_data[[year]]
#       site_list <- map(names(calibration_data[[year]]), ~word(.x, 1, sep = "-")) %>% 
#         unlist() %>% 
#         unique(.) 
#       
#       site_list <- unique(c("tamasag", "legacy", "lincoln", "timberline", "prospect", "boxelder", "archery", "river bluffs", site_list)) 
#       
#       # Generate start and end times for the HydroVu API request
#       start_dt_string <- paste0(year, "-01-01 00:00:00")
#       start_dt = as.POSIXct(start_dt_string, tz = "America/Denver")
#       start_dt_utc = with_tz(start_dt, tzone = "UTC")
#       
#       end_dt_string <- paste0(year, "-12-31 11:59:59")
#       end_dt = as.POSIXct(end_dt_string, tz = "America/Denver")
#       end_dt_utc = with_tz(end_dt, tzone = "UTC")
#       
#       api_start_dates <- tibble(site = site_list,
#                                 start_DT = start_dt_utc,
#                                 end_DT = end_dt_utc)
#       
#       # Grab HydroVu Credits
#       hv_creds <- read_yaml(here("creds", "HydroVuCreds.yml"))
#       
#       hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
#                           client_secret = as.character(hv_creds["secret"]))
#       
#       hv_sites <- hv_locations_all(hv_token) %>% 
#         filter(!grepl("vulink|virridy", name, ignore.case = TRUE),
#                grepl(str_flatten(site_list, collapse = "|"), name, ignore.case = TRUE))
#       
#       # Create temporary directory
#       temp_dir <- file.path(tempdir(), paste0("api_data_", year))
#       dir.create(temp_dir, showWarnings = FALSE)
#       temp_dir <- normalizePath(temp_dir)
#       
#       # Pull the API data
#       pwalk(api_start_dates,
#             function(site, start_DT, end_DT) {
#               message("Requesting HydroVu data for site: ", site)
#               fcw.qaqc::api_puller(
#                 site = site,
#                 start_dt = start_DT,
#                 end_dt = end_DT,
#                 api_token = hv_token,
#                 hv_sites_arg = hv_sites,
#                 # Use temporary directory for the API call
#                 dump_dir = temp_dir,
#                 synapse_env = FALSE,
#                 fs = NULL
#               )
#             }
#       )
#       
#       # Munge the API data
#       message("Starting Munge Step")
#       
#       if (length(list.files(temp_dir)) == 0) {
#         return(NULL)
#       }
#       
#       new_data <- fcw.qaqc::munge_api_data(api_dir = temp_dir) %>% 
#         fix_sites() %>%
#         split(f = list(.$site, .$parameter), sep = "-") %>%
#         purrr::keep(~ nrow(.) > 0)
#       
#       # Tidy the API data
#       message("Starting Tidy Step")
#       sites <- unique(dplyr::bind_rows(new_data) %>% dplyr::pull(site))
#       
#       params <- c("Chl-a Fluorescence", "Depth", "DO", "ORP", "pH", "Specific Conductivity",
#                   "Pressure", "Turbidity")
#       
#       site_param_combos <- crossing(sites, params) %>% 
#         mutate(combo = paste0(sites, "-", params)) %>% 
#         pull(combo) 
#       
#       new_data <- new_data[names(new_data) %in% site_param_combos]
#       
#       sensor_data <- map(new_data, ~fcw.qaqc::tidy_api_data(api_data = .x)) %>% 
#         keep(~!is.null(.))
#       
#       # Save the API data...
#       return(sensor_data)
#     })
#   
#   # Save the raw data into a file that we can pull in in the future...
#   write_rds(sensor_data, here("data", "manual_data_verification", "complete_dataset", "23_25_hv_pull"))
# }

```

## Expected Sensor Data Structure

The sensor data must be organized as a year list structure where each year contains 
site-parameter lists. For example, the 2024 data would contain separate entries 
for "lincoln-pH", "lincoln-Turbidity", "tamasag-pH", etc. Each site-parameter 
combination contains time series data with datetime stamps and sensor measurements. 
This consistent structure allows the calibration functions to properly match 
sensor readings with their corresponding calibration information.

# Processing steps

## Join Sensor and Calibration Data

```{r join-data}
sensor_calibration_data <- cal_join_sensor_calibration_data(
  # TODO: Join with field visits and make some elastic join based on the most recent site visit
  # Sometimes the calibration date does not line up with a field visit.
  sensor_data_list = sensor_data,
  calibration_data_list = calibration_data
) 
```

The `join_sensor_calibration_data()` function links each sensor measurement to 
its appropriate calibration information using temporal proximity matching. This 
function identifies the most recent calibration that applies to each sensor 
reading, creating a foundation for the back-calibration process.

## Prepare Data for Back Calibration

UPDATE (JPD): We will have to join the sensor provenance data here in the workflow.

There are some notes in the updated `cal_prepare_sensor_calibration_data` function,
but once this is finalized those will be gone. This will also get cleaned up,
but I want to explain what I did so that it can be critiqued easily. Some version 
of this will also be in the draft PR.

Okay. The previous solution had us binding consecutive "calibration chunks" together
so that we would be able to go from one calibration to the next. This update simplifies
this process a lot. Instead of doing that we make the `cal_provenance_df` object
that has a "lead" sensor calibration date and calibration coefs column that we use 
to join with the calibration chunks that we make in `cal_prepare_sensor_calibration_data`.
The code updates that follow just require pointing at a new part of the df that 
gets passed into the functions that are called in cal_back calibrate.

```{r prep-data}
prepped_snsr_cal_data <- cal_prepare_sensor_calibration_data(
  sensor_calibration_data_list = sensor_calibration_data,
  cal_prov_df = cal_provenance_df
)
```

The `prepare_sensor_calibration_data()` function segments the joined data into 
calibration windows bounded by consecutive calibrations. Each window represents 
a time period where linear interpolation between two calibrations can be applied. 
This segmentation creates manageable chunks that allow for temporal weighting of 
calibration parameters.

# Back Calibration

```{r Apply back calibration functions to the data}
calibrated_data <- prepped_snsr_cal_data %>% 
  map(function(year){
    calibrated_site_param_list <- year %>%
      map(function(site_param){
        site_param %>% 
          map_dfr(function(chunk){
            cal_back_calibrate(chunk) 
          })
      })
  })
```

## Understanding the Nested Map Structure

The back calibration process uses an explicit nested map approach that differs 
from the simpler function calls used in previous processing steps. This design 
choice provides three levels of iteration: years, site-parameters within each 
year, and calibration chunks within each site-parameter. While `join_sensor_calibration_data()` 
and `prepare_sensor_calibration_data()` handle similar nested year/site-parameter 
iterations internally, they abstract away this complexity from the user.

The explicit nested structure in  the `back_calibration()` function call serves 
a purpose for future functionality. While automated calibration works well for 
most situations, some sensor data may require manual intervention or custom 
calibration approaches. The exposed nested maps allow users to easily extract 
specific years, site-parameters, or individual chunks for manual processing 
while maintaining the overall automated workflow for standard cases.

## The `back_calibrate()` Function

The `back_calibrate()` function serves as the orchestrator for the entire calibration 
process within each chunk. It automatically determines the appropriate calibration 
workflow based on the sensor parameter type. Standard sensors (Chlorophyll-a, 
FDOM, ORP, Pressure, Specific Conductivity, RDO) follow a five-step process 
involving temporal weighting, inverse linear modeling, linear transformation, 
single-point drift correction, and validation. Turbidity sensors use a similar 
workflow but with two-point drift correction, while pH sensors require specialized 
three-point processing due to their multi-segment calibration approach.

This function design ensures that users only need to call one function per chunk 
while the internal logic handles the complexity of parameter-specific calibration 
requirements.

# Saving the Back-calibrated Data

```{r save-cal-data}
write_rds(calibrated_data, here("data", "manual_data_verification", "complete_dataset", "calibrated_sensor_data.rds"))
```

# Explore the calibrations

```{r explore-cal}
# update `calibrated_test` object with the data you would like to explore...
calibrated_test <- calibrated_data$`2024`$`cottonwood-Turbidity`

# Extract parameter and site info for dynamic labeling
parameter <- unique(calibrated_test$parameter)
site <- unique(calibrated_test$site)

# Create ggplot with basic labels and legend
calibration_p_test <- ggplot(calibrated_test) +
  geom_line(aes(x = DT_round, y = mean, color = "Original"), alpha = 0.6) +
  geom_line(aes(x = DT_round, y = mean_raw, color = "Raw"), alpha = 0.3, linetype = "dotted") +
  geom_line(aes(x = DT_round, y = mean_drift_trans, color = "Drift Corrected"), alpha = 0.6) +
  geom_line(aes(x = DT_round, y = mean_lm_trans, color = "Linear Transform"), alpha = 0.6) +
  scale_color_manual(values = c("Original" = "red", "Raw" = "yellow", 
                                "Linear Transform" = "green", "Drift Corrected" = "blue")) +
  labs(
    title = paste(site, parameter, "2024 Calibration"),
    x = "Date",
    y = paste(parameter, "(units)"),
    color = "Data Type"
  ) +
  ylim(c(0,50)) +
  theme_minimal()

# Convert to plotly
calibration_p_test <- ggplotly(calibration_p_test)

# Add vertical lines for calibration dates
vline_dates <- round_date(unique(calibrated_test$file_date), "15 minutes")
for(date in vline_dates) {
  calibration_p_test <- calibration_p_test %>% add_segments(x = date, xend = date, 
                                                            y = min(calibrated_test$mean_drift_trans, na.rm = TRUE), 
                                                            yend = max(calibrated_test$mean_drift_trans, na.rm = TRUE), 
                                                            line = list(color = "black", dash = "dash"),
                                                            showlegend = FALSE)
}

# Display the plot
calibration_p_test <- ggplotly(calibration_p_test)

calibration_p_test
```

# Workflow Summary
The complete calibration workflow transforms raw sensor data through four main stages. 
First, calibration information is loaded and joined with sensor data using temporal 
matching. Second, the joined data is segmented into calibration windows for processing. 
Third, each window undergoes back calibration using parameter-specific algorithms 
that account for instrument characteristics and drift patterns. Finally, the calibrated 
results are validated and compiled into a clean time series dataset ready for analysis.
